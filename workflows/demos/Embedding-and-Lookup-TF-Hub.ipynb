{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Hub Embedding and Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['VERTA_EMAIL'] = ''\n",
    "# os.environ['VERTA_DEV_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"dev.verta.ai\"\n",
    "\n",
    "PROJECT_NAME = \"TFHub Train Example\"\n",
    "EXPERIMENT_NAME = \"Embed for Training\"\n",
    "\n",
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST)\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '/Users/convoliution/Downloads/'\n",
    "DATA_FILE = DATA_DIR + 'imdb_master.csv'\n",
    "\n",
    "reviews = pd.read_csv(DATA_FILE, encoding='latin')['review'].values.tolist()\n",
    "\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class EmbeddingOnlyModel(): \n",
    "    '''\n",
    "    This implementation assumes that the ANN index is served by a different service\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.EMBEDDING_LENGTH = 512 \n",
    "        os.environ[\"TFHUB_CACHE_DIR\"] = \"tf_cache_dir\"\n",
    "        \n",
    "        # define the graph\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            self.text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "            self.encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "            self.my_result = self.encoder(self.text_input)\n",
    "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        g.finalize()\n",
    "        self.graph = g\n",
    "        \n",
    "        # Create session and initialize.\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        self.session.run(init_op)\n",
    "        \n",
    "    def predict(self, sentences):\n",
    "        return self.session.run(self.my_result, feed_dict={self.text_input: sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_only_model = EmbeddingOnlyModel()\n",
    "embedding_only_model.predict([\"I love this movie\", \"I love this movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import annoy\n",
    "class EmbeddingAndLookupModel():\n",
    "    '''\n",
    "    This class assumes that the ANN index is available within the model class\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.EMBEDDING_LENGTH = 512 \n",
    "        os.environ[\"TFHUB_CACHE_DIR\"] = \"tf_cache_dir\"\n",
    "        \n",
    "        # define the graph\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            self.text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "            self.encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "            self.my_result = self.encoder(self.text_input)\n",
    "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        g.finalize()\n",
    "        self.graph = g\n",
    "        \n",
    "        # Create session and initialize.\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        self.session.run(init_op)\n",
    "        \n",
    "        # load the index\n",
    "        self.index = annoy.AnnoyIndex(self.EMBEDDING_LENGTH, \"angular\")\n",
    "        self.index.load(\"reviews.ann\") # available locally or as artifact/custom module\n",
    "        \n",
    "    def compute_single_embedding(self, sentence):\n",
    "        return self.session.run(self.my_result, feed_dict={self.text_input: [sentence]})\n",
    "     \n",
    "    def predict(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            # embed the sentence\n",
    "            embedding = self.session.run(self.my_result, feed_dict={self.text_input: [sentence]})\n",
    "\n",
    "            # find closest\n",
    "            predictions = []\n",
    "            predictions.append({\n",
    "                sentence: self.index.get_nns_by_vector(embedding[0], 10)\n",
    "            })\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = EmbeddingAndLookupModel()\n",
    "predict2.predict([\"i love this movie\", \"i love this movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildEmbeddingLookupIndex():\n",
    "    '''\n",
    "    This train step creates embeddings from text and puts them in an index\n",
    "    \n",
    "    TODO: upload the index as an artifact for EmbeddingAndLookupModel\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.EMBEDDING_LENGTH = 512\n",
    "        import os\n",
    "        os.environ[\"TFHUB_CACHE_DIR\"] = \"tf_cache_dir\"\n",
    "        \n",
    "        # define the graph\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            self.text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "            self.encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "            self.my_result = self.encoder(self.text_input)\n",
    "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        g.finalize()\n",
    "        self.graph = g\n",
    "        \n",
    "        # Create session and initialize.\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        self.session.run(init_op)\n",
    "        \n",
    "    def compute_index(self, sentences):\n",
    "        from annoy import AnnoyIndex\n",
    "        t = AnnoyIndex(self.EMBEDDING_LENGTH, 'angular')  # Length of item vector that will be indexed\n",
    "        for i in range(len(sentences)):\n",
    "            # calculate embedding with TF\n",
    "            embedding = self.session.run(self.my_result, feed_dict={self.text_input: [sentences[i]]})\n",
    "            t.add_item(i, embedding[0])\n",
    "\n",
    "        t.build(10) # 10 trees\n",
    "        t.save('reviews.ann')\n",
    "        \n",
    "    def compute_single_embedding(self, sentence):\n",
    "        return self.session.run(self.my_result, feed_dict={self.text_input: [sentence]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_index = BuildEmbeddingLookupIndex()\n",
    "print(build_index.compute_single_embedding(\"I love this movie\").shape)\n",
    "build_index.compute_index(reviews[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
