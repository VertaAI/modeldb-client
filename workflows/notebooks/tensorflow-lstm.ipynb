{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Recurrent Network (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"data\", \"imdb\")\n",
    "DATA_FILE = \"imdb.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join(DATA_DIR, DATA_FILE))\n",
    "\n",
    "# gather indices to split training data into training and validation sets\n",
    "data_train = (data['x_train'], data['y_train'])\n",
    "shuffled_idxs = np.random.permutation(data['x_train'].shape[0])\n",
    "idxs_train = shuffled_idxs[len(shuffled_idxs)//10:]  # last 90%\n",
    "idxs_val = shuffled_idxs[:len(shuffled_idxs)//10]  # first 10%\n",
    "\n",
    "x_train, y_train = data['x_train'][idxs_train], data['y_train'][idxs_train]\n",
    "x_val, y_val = data['x_train'][idxs_val], data['y_train'][idxs_val]\n",
    "x_test, y_test = data['x_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"imdb_word_index.json\")) as f:\n",
    "    word_index = json.load(f)\n",
    "    \n",
    "# add special tokens\n",
    "word_index = {word: index+3 for word, index in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad input sequences\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                     value=word_index[\"<PAD>\"],\n",
    "                                                     padding='post',\n",
    "                                                     maxlen=300)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val,\n",
    "                                                   value=word_index[\"<PAD>\"],\n",
    "                                                   padding='post',\n",
    "                                                   maxlen=300)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                    value=word_index[\"<PAD>\"],\n",
    "                                                    padding='post',\n",
    "                                                    maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(max(word_index.values())+1, 16))\n",
    "model.add(keras.layers.LSTM(32, return_sequences=True))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "22500/22500 [==============================] - 65s 3ms/step - loss: 0.7016 - acc: 0.5267 - val_loss: 0.6801 - val_acc: 0.5424\n",
      "Epoch 2/10\n",
      "22500/22500 [==============================] - 68s 3ms/step - loss: 0.6646 - acc: 0.5831 - val_loss: 0.7239 - val_acc: 0.5644\n",
      "Epoch 3/10\n",
      "22500/22500 [==============================] - 58s 3ms/step - loss: 0.5769 - acc: 0.6864 - val_loss: 0.5466 - val_acc: 0.7580\n",
      "Epoch 4/10\n",
      "22500/22500 [==============================] - 57s 3ms/step - loss: 0.6550 - acc: 0.6150 - val_loss: 0.6773 - val_acc: 0.5444\n",
      "Epoch 5/10\n",
      "22500/22500 [==============================] - 59s 3ms/step - loss: 0.6014 - acc: 0.6591 - val_loss: 0.5709 - val_acc: 0.7456\n",
      "Epoch 6/10\n",
      "22500/22500 [==============================] - 63s 3ms/step - loss: 0.4494 - acc: 0.7980 - val_loss: 0.4774 - val_acc: 0.7972\n",
      "Epoch 7/10\n",
      "22500/22500 [==============================] - 65s 3ms/step - loss: 0.3333 - acc: 0.8658 - val_loss: 0.4091 - val_acc: 0.8368\n",
      "Epoch 8/10\n",
      "22500/22500 [==============================] - 74s 3ms/step - loss: 0.2515 - acc: 0.9090 - val_loss: 0.4228 - val_acc: 0.8380\n",
      "Epoch 9/10\n",
      "22500/22500 [==============================] - 62s 3ms/step - loss: 0.2153 - acc: 0.9245 - val_loss: 0.4394 - val_acc: 0.8364\n",
      "Epoch 10/10\n",
      "22500/22500 [==============================] - 71s 3ms/step - loss: 0.1911 - acc: 0.9329 - val_loss: 0.4016 - val_acc: 0.8448\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 109s 5ms/step\n",
      "Training accuracy: 0.9525777777777777\n",
      "25000/25000 [==============================] - 110s 4ms/step\n",
      "Testing accuracy: 0.84356\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {model.evaluate(x_train, y_train)[1]}\")\n",
    "print(f\"Testing accuracy: {model.evaluate(x_test, y_test)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "keras.models.save_model(model, os.path.join(\"..\", \"output\", \"tensorflow-lstm.hdf5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
