{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkTLZ3I4_7c_"
   },
   "source": [
    "# Sentence Pair Classfication with BERT using a Cloud TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wtjs1QDb3DX"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**BERT**, or Bidirectional Embedding Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. The academic paper can be found here: https://arxiv.org/abs/1810.04805.\n",
    "\n",
    "This Colab demonstates the steps of fine-tuning a sentence and sentence-pair classification tasks built on top of pretrained BERT models (from  [TF Hub](https://www.tensorflow.org/hub)) and \n",
    "run predictions on tuned model.\n",
    "\n",
    "*This notebook also requires you to have a Google account and access to a GCP Bucket.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kNzDlH4SGN9"
   },
   "source": [
    "## Setting up Verta for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHLLOdKjSKFW"
   },
   "outputs": [],
   "source": [
    "#installing vertax - restart your notebook if prompted\n",
    "try:\n",
    "    import verta\n",
    "except ModuleNotFoundError:\n",
    "    !pip install verta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM5EsWLLSJ7n"
   },
   "outputs": [],
   "source": [
    "HOST = \"demo.app.verta.ai\"\n",
    "PROJECT_NAME = \"BERT-Classification\"\n",
    "EXPERIMENT_NAME = \"Sentence-Pair-Clf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3p0fs6qSJ0K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VERTA_EMAIL'] =  \"\"\n",
    "os.environ['VERTA_DEV_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPV2mA_5SJq8"
   },
   "outputs": [],
   "source": [
    "from verta import Client\n",
    "from verta.utils import ModelAPI\n",
    "\n",
    "client = Client(HOST,\n",
    "                use_git=False)\n",
    "\n",
    "proj = client.set_project(PROJECT_NAME)\n",
    "expt = client.set_experiment(EXPERIMENT_NAME)\n",
    "run = client.set_experiment_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ld-JXlueIuPH"
   },
   "source": [
    "## Set up your TPU environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdMmwCJFaT8F"
   },
   "source": [
    "\n",
    "\n",
    "In this section, you perform the following tasks:\n",
    "\n",
    "*   Set up a Colab TPU running environment\n",
    "*   Verify that you are connected to a TPU device\n",
    "*   Upload your credentials to TPU to access your GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "191zq3ZErihP"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import predictor\n",
    "\n",
    "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
    "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "print('TPU address is', TPU_ADDRESS)\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "with tf.Session(TPU_ADDRESS) as session:\n",
    "    print('TPU devices:')\n",
    "    pprint.pprint(session.list_devices())\n",
    "\n",
    "    # Upload credentials to TPU.\n",
    "    with open('/content/adc.json', 'r') as f:\n",
    "        auth_info = json.load(f)\n",
    "        tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
    "    # Now credentials are set for all future sessions on this TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUBP35oCDmbF"
   },
   "source": [
    "### Prepare and import BERT modules\n",
    "â€‹\n",
    "With your environment configured, you can now prepare and import the BERT modules. The following step clones the source code from GitHub and import the modules from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wzwke0sxS6W"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# bert repository is created and managed by google research\n",
    "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
    "if not 'bert_repo' in sys.path:\n",
    "    sys.path += ['bert_repo']\n",
    "\n",
    "# import python modules defined by BERT\n",
    "import modeling\n",
    "import optimization\n",
    "import run_classifier\n",
    "import run_classifier_with_tfhub\n",
    "import tokenization\n",
    "\n",
    "# import tfhub \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRu1aKO1D7-Z"
   },
   "source": [
    "### Prepare for training\n",
    "\n",
    "This next section of code performs the following tasks:\n",
    "\n",
    "*  Specify task and download training data.\n",
    "    - Using the standard GLUE training set from Microsoft Research for our example. Find more information [here](https://aclweb.org/aclwiki/Paraphrase_Identification_(State_of_the_art).\n",
    "*  Specify BERT pretrained model\n",
    "*  Specify GS bucket to download the model\n",
    "*  Create output directory for model checkpoints and eval results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "tYkaAlJNfhul"
   },
   "outputs": [],
   "source": [
    "TASK = 'MRPC'\n",
    "\n",
    "# Download glue data.\n",
    "!test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo\n",
    "!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=$TASK\n",
    "\n",
    "TASK_DATA_DIR = 'glue_data/' + TASK\n",
    "print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
    "!ls $TASK_DATA_DIR\n",
    "\n",
    "BUCKET = 'YOUR_BUCKET-NAME'\n",
    "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
    "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
    "\n",
    "# Available pretrained model checkpoints:\n",
    "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
    "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
    "#   cased_L-12_H-768_A-12: cased BERT large model\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hcpfl4N2EdOk"
   },
   "source": [
    "Now let's load tokenizer module from TF Hub and play with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TylDOYd-6AH-"
   },
   "outputs": [],
   "source": [
    "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqQ_LHyzcoYW"
   },
   "source": [
    "Also we initilize our hyperprams, prepare the training data and initialize TPU config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kl27m-vlbDk1"
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 8\n",
    "PREDICT_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500\n",
    "\n",
    "run.log_hyperparameter(\"train_batch_size\", TRAIN_BATCH_SIZE)\n",
    "run.log_hyperparameter(\"eval_batch_size\", EVAL_BATCH_SIZE)\n",
    "run.log_hyperparameter(\"predict_batch_size\", PREDICT_BATCH_SIZE)\n",
    "run.log_hyperparameter(\"learning_rate\", LEARNING_RATE)\n",
    "run.log_hyperparameter(\"num_train_epochs\", NUM_TRAIN_EPOCHS)\n",
    "run.log_hyperparameter(\"warmup_proportion\", WARMUP_PROPORTION)\n",
    "run.log_hyperparameter(\"save_checkpoint_steps\", SAVE_CHECKPOINTS_STEPS)\n",
    "run.log_hyperparameter(\"save_summary_steps\", SAVE_SUMMARY_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYVYULZiKvUi"
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "  \"cola\": run_classifier.ColaProcessor,\n",
    "  \"mnli\": run_classifier.MnliProcessor,\n",
    "  \"mrpc\": run_classifier.MrpcProcessor,\n",
    "}\n",
    "processor = processors[TASK.lower()]()\n",
    "label_list = processor.get_labels()\n",
    "print(\"LABELS: \", label_list)\n",
    "\n",
    "# Compute number of train and warmup steps from batch size\n",
    "train_examples = processor.get_train_examples(TASK_DATA_DIR)\n",
    "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Setup TPU related config\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "NUM_TPU_CORES = 8\n",
    "ITERATIONS_PER_LOOP = 1000\n",
    "\n",
    "def get_run_config(output_dir):\n",
    "    return tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=output_dir,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3iFMeqLaSll"
   },
   "source": [
    "# Fine-tune and Run Predictions on a pretrained BERT Model from TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXyJRc0OIHEU"
   },
   "source": [
    "This section demonstrates fine-tuning from a pre-trained BERT TF Hub module and running predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwcsdbLuIX2I"
   },
   "outputs": [],
   "source": [
    "# Force TF Hub writes to the GS bucket we provide.\n",
    "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
    "\n",
    "model_fn = run_classifier_with_tfhub.model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  use_tpu=True,\n",
    "  bert_hub_module_handle=BERT_MODEL_HUB\n",
    ")\n",
    "\n",
    "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
    "  use_tpu=True,\n",
    "  model_fn=model_fn,\n",
    "  config=get_run_config(OUTPUT_DIR),\n",
    "  train_batch_size=TRAIN_BATCH_SIZE,\n",
    "  eval_batch_size=EVAL_BATCH_SIZE,\n",
    "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGVYj_rlcDhc"
   },
   "source": [
    "At this point, you can now fine-tune the model, evaluate it, and run predictions on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5U_c8s2AvhgL"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_train(estimator):\n",
    "    print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
    "    # We'll set sequences to be at most 128 tokens long.\n",
    "    train_features = run_classifier.convert_examples_to_features(\n",
    "        train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "    print('  Num examples = {}'.format(len(train_examples)))\n",
    "    print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    train_input_fn = run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRhzyk4_WD2n"
   },
   "outputs": [],
   "source": [
    "model_train(estimator_from_tfhub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoXRtSPZvdiS"
   },
   "outputs": [],
   "source": [
    "def model_eval(estimator):\n",
    "    # Eval the model.\n",
    "    eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
    "    eval_features = run_classifier.convert_examples_to_features(\n",
    "        eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "    print('  Num examples = {}'.format(len(eval_examples)))\n",
    "    print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
    "\n",
    "    # Eval will be slightly WRONG on the TPU because it will truncate\n",
    "    # the last batch.\n",
    "    eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
    "    eval_input_fn = run_classifier.input_fn_builder(\n",
    "        features=eval_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=True)\n",
    "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "    print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "    output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
    "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            print('  {} = {}'.format(key, str(result[key])))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLQehBr4WHjj"
   },
   "outputs": [],
   "source": [
    "model_eval(estimator_from_tfhub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3_afEMR3dfa"
   },
   "outputs": [],
   "source": [
    "def model_predict(estimator):\n",
    "    # Make predictions on a subset of eval examples\n",
    "    prediction_examples = processor.get_dev_examples(TASK_DATA_DIR)[:PREDICT_BATCH_SIZE]\n",
    "    input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "\n",
    "    for example, prediction in zip(prediction_examples, predictions):\n",
    "        print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\n' % (example.text_a, example.text_b, str(example.label), prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynDmeatCWLJK"
   },
   "outputs": [],
   "source": [
    "model_predict(estimator_from_tfhub) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GrEptzvDaUI"
   },
   "source": [
    "## Saving and Deploying the BERT model\n",
    "In this section performs the following tasks:\n",
    "* Saving the model for predictions\n",
    "* Creating a wrapper for deployment\n",
    "* Logging models on Verta for deployment\n",
    "* Predict on text from the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIjujOYFD3aX"
   },
   "source": [
    "Tensorflow provides a more efficient way of serializing any inference graph that plays nicely with the rest of the ecosystem, like Tensorflow Serving. In line with the tf.estimator technical specifications of making it an easy-to-use, high-level API, exporting an Estimator as a saved_model is really simple.\n",
    "\n",
    "We first need to define a special `input_fn`. Then, reloading and serializing the estimator is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "78MUftvzhmpL",
    "outputId": "248cc2d6-e553-4e68-9be9-49f735d58723"
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
    "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "        'label_ids': label_ids,\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "    })()\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iG9rt_rqjRgK"
   },
   "outputs": [],
   "source": [
    "estimator_from_tfhub._export_to_tpu = False\n",
    "estimator_from_tfhub.export_saved_model('path/to/save/model', serving_input_fn) # saved to your local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_model('trained_model', 'path/to/save/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZtMShTPGCNe"
   },
   "source": [
    "### Creating a wrapper class\n",
    "Verta deployment expects a particular interface for its models.\n",
    "We must expose a predict() function based on the code we wrote earlier. This becomes a thin wrapper around our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrU-zAJJkUHa"
   },
   "outputs": [],
   "source": [
    "# Testing prediction\n",
    "dummy_example = {\"input_ids\":np.zeros((1,128), dtype=int).tolist(),\n",
    "           \"input_mask\":np.zeros((1,128), dtype=int).tolist(),\n",
    "           \"label_ids\":[0],\n",
    "           \"segment_ids\":np.zeros((1,128), dtype=int).tolist()}\n",
    "\n",
    "latest = '/content/saved_model/1564784206'\n",
    "predict_fn = predictor.from_saved_model(latest)\n",
    "results = predict_fn(example)['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HiKVDkqgnWJ"
   },
   "outputs": [],
   "source": [
    "model_api = ModelAPI(in_sentences, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDGnkxNPg_PY"
   },
   "outputs": [],
   "source": [
    "import six\n",
    "# this is like open(\"requirements.txt\"), but without creating a new file\n",
    "requirements = six.StringIO('\\n'.join([\n",
    "    \"tensorflow=={}\".format(tf.__version__),\n",
    "    \"tensorflow-hub=={}\".format(hub.__version__),\n",
    "    \"bert-tensorflow==1.0.1\",\n",
    "    \"numpy=={}\".format(np.__version__)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aufiRP_9GAfg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import predictor\n",
    "import numpy as np\n",
    "\n",
    "class BertSentenceClf:\n",
    "    def __init__(self, path):\n",
    "        self.predict_fn = predictor.from_saved_model(path)\n",
    "    \n",
    "    def predict(self, in_sentences):\n",
    "        input_examples = [run_classifier.InputExample(guid=\"\", text_a = x[0], text_b = x[1], label = \"0\") for x in in_sentences] # here, \"\" is just a dummy label\n",
    "        input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "        # predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
    "        predictions = []\n",
    "        for i in input_features:\n",
    "            print()\n",
    "            res = self.predict_fn({\"input_ids\":i.input_ids,\n",
    "                                   \"input_mask\":i.input_mask,\n",
    "                                   \"label_ids\":[i.label_id],\n",
    "                                   \"segment_ids\":i.segment_ids})\n",
    "            predictions.append(res['probabilities'])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-butkFRa3F-"
   },
   "outputs": [],
   "source": [
    "# fails - mismatch in tensors\n",
    "in_sentences = [(\"This integrates with Rational PurifyPlus and allows developers to work in supported versions of Java , Visual C # and Visual Basic .NET.\",\n",
    "                   \"IBM said the Rational products were also integrated with Rational PurifyPlus , which allows developers to work in Java , Visual C # and VisualBasic .Net.\")]\n",
    "model = BertSentenceClf(path= '/path/to/saved/model/')  \n",
    "model.predict(in_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAejIiM8huLu"
   },
   "outputs": [],
   "source": [
    "# TypeError: can't pickle _thread.RLock objects\n",
    "run.log_model_for_deployment(model, model_api, requirements)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT : Sentence and Sentence-Pair Classification Tasks",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
